#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºäºYOLOv8 Poseçš„å¤šäººå®æ—¶å§¿æ€åŠ¨ä½œè¯†åˆ«ç³»ç»Ÿ
ä½¿ç”¨ultralytics YOLOv8è¿›è¡Œæ›´å‡†ç¡®çš„å¤šäººå…³é”®ç‚¹æ£€æµ‹
"""

import cv2
import numpy as np
import time
import yaml
from collections import deque, defaultdict
from ultralytics import YOLO
import torch
from behavior_model import AdvancedActionRecognizer, ConsoleActionLogger, FileActionLogger
import math
from typing import List, Dict, Tuple, Optional


class YOLOMultiPersonRecognizer:
    """åŸºäºYOLOv8çš„å¤šäººå§¿æ€è¯†åˆ«å™¨"""
    
    def __init__(self, config_path: str = 'rule_config.yaml', model_path: str = 'yolov8n-pose.pt'):
        # åŠ è½½é…ç½®
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        # åˆå§‹åŒ–åŠ¨ä½œè¯†åˆ«å™¨
        self.action_recognizer = AdvancedActionRecognizer()
        
        # åˆå§‹åŒ–YOLOv8 Poseæ¨¡å‹
        print(f"ğŸ¤– åŠ è½½YOLOv8 Poseæ¨¡å‹: {model_path}")
        self.yolo_model = YOLO(model_path)
        
        # æ£€æŸ¥GPUå¯ç”¨æ€§
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # æ¨¡å‹å‚æ•°
        self.conf_threshold = 0.5  # ç½®ä¿¡åº¦é˜ˆå€¼
        self.iou_threshold = 0.7   # NMSé˜ˆå€¼
        
        # æ€§èƒ½ç»Ÿè®¡
        self.fps_history = deque(maxlen=30)
        self.frame_count = 0
        self.start_time = time.time()
        
        # çŠ¶æ€ç®¡ç†
        self.is_running = False
        self.tracked_persons = {}  # è·Ÿè¸ªçš„äººå‘˜
        self.person_id_counter = 0
        
        # åŠ¨ä½œæ˜ å°„ï¼ˆè§£å†³ä¸­æ–‡ä¹±ç ï¼‰
        self.action_mapping = {
            "ç«™ç«‹": "Standing",
            "å": "Sitting", 
            "èººä¸‹": "Lying",
            "æ‰“çŒç¡": "Dozing",
            "æœªçŸ¥": "Unknown"
        }
        
        # é¢œè‰²å®šä¹‰ï¼ˆæ›´å¤šé¢œè‰²ï¼‰
        self.colors = [
            (0, 255, 0),    # ç»¿è‰²
            (255, 0, 0),    # è“è‰²
            (0, 0, 255),    # çº¢è‰²
            (255, 255, 0),  # é’è‰²
            (255, 0, 255),  # æ´‹çº¢
            (0, 255, 255),  # é»„è‰²
            (128, 255, 128), # æµ…ç»¿
            (255, 165, 0),  # æ©™è‰²
            (128, 0, 128),  # ç´«è‰²
            (255, 192, 203), # ç²‰è‰²
        ]
        
        # äººå‘˜è·Ÿè¸ªå‚æ•°
        self.tracking_threshold = 150  # åƒç´ è·ç¦»é˜ˆå€¼
        self.max_missing_frames = 15   # æœ€å¤§ä¸¢å¤±å¸§æ•°
        
        # COCOå§¿æ€å…³é”®ç‚¹å®šä¹‰ï¼ˆYOLOv8ä½¿ç”¨COCOæ ¼å¼ï¼‰
        self.coco_keypoint_names = [
            'nose', 'left_eye', 'right_eye', 'left_ear', 'right_ear',
            'left_shoulder', 'right_shoulder', 'left_elbow', 'right_elbow',
            'left_wrist', 'right_wrist', 'left_hip', 'right_hip',
            'left_knee', 'right_knee', 'left_ankle', 'right_ankle'
        ]
        
        # éª¨æ¶è¿æ¥å®šä¹‰
        self.skeleton_connections = [
            (0, 1), (0, 2), (1, 3), (2, 4),  # å¤´éƒ¨
            (5, 6), (5, 7), (7, 9), (6, 8), (8, 10),  # ä¸Šèº«
            (5, 11), (6, 12), (11, 12),  # èº¯å¹²
            (11, 13), (13, 15), (12, 14), (14, 16)  # ä¸‹èº«
        ]
    
    def _yolo_to_coco_keypoints(self, yolo_keypoints: np.ndarray) -> List[float]:
        """
        å°†YOLOv8å…³é”®ç‚¹è½¬æ¢ä¸ºCOCOæ ¼å¼
        YOLOv8è¾“å‡ºæ ¼å¼: [x1, y1, conf1, x2, y2, conf2, ...]  (17ä¸ªå…³é”®ç‚¹)
        COCOæ ¼å¼: [x1, y1, conf1, x2, y2, conf2, ...]  (17ä¸ªå…³é”®ç‚¹ Ã— 3 = 51ä¸ªå€¼)
        """
        if yolo_keypoints is None or len(yolo_keypoints) == 0:
            return [0.0] * 51
        
        # YOLOv8 poseè¾“å‡ºçš„å…³é”®ç‚¹å·²ç»æ˜¯COCOæ ¼å¼ï¼Œåªéœ€ç¡®ä¿é•¿åº¦æ­£ç¡®
        keypoints = yolo_keypoints.flatten().tolist()
        
        # ç¡®ä¿æœ‰51ä¸ªå€¼ï¼ˆ17ä¸ªå…³é”®ç‚¹ Ã— 3ï¼‰
        if len(keypoints) < 51:
            keypoints.extend([0.0] * (51 - len(keypoints)))
        elif len(keypoints) > 51:
            keypoints = keypoints[:51]
        
        return keypoints
    
    def _get_person_center(self, keypoints: np.ndarray) -> Optional[Tuple[float, float]]:
        """è·å–äººå‘˜ä¸­å¿ƒç‚¹ï¼ˆåŸºäºè‚©è†€å’Œé«‹éƒ¨ï¼‰"""
        if keypoints is None or len(keypoints) < 51:
            return None
        
        # é‡å¡‘ä¸º (17, 3) å½¢çŠ¶
        kpts = keypoints.reshape(17, 3)
        
        # ä½¿ç”¨è‚©è†€å’Œé«‹éƒ¨è®¡ç®—ä¸­å¿ƒç‚¹
        key_points = []
        
        # å·¦å³è‚©è†€ (ç´¢å¼•5, 6)
        if kpts[5, 2] > 0.3:  # å·¦è‚©ç½®ä¿¡åº¦
            key_points.append((kpts[5, 0], kpts[5, 1]))
        if kpts[6, 2] > 0.3:  # å³è‚©ç½®ä¿¡åº¦
            key_points.append((kpts[6, 0], kpts[6, 1]))
        
        # å·¦å³é«‹éƒ¨ (ç´¢å¼•11, 12)
        if kpts[11, 2] > 0.3:  # å·¦é«‹ç½®ä¿¡åº¦
            key_points.append((kpts[11, 0], kpts[11, 1]))
        if kpts[12, 2] > 0.3:  # å³é«‹ç½®ä¿¡åº¦
            key_points.append((kpts[12, 0], kpts[12, 1]))
        
        if key_points:
            center_x = sum(p[0] for p in key_points) / len(key_points)
            center_y = sum(p[1] for p in key_points) / len(key_points)
            return (center_x, center_y)
        
        return None
    
    def _calculate_distance(self, point1: Optional[Tuple[float, float]], 
                          point2: Optional[Tuple[float, float]]) -> float:
        """è®¡ç®—ä¸¤ç‚¹è·ç¦»"""
        if point1 is None or point2 is None:
            return float('inf')
        return math.sqrt((point1[0] - point2[0])**2 + (point1[1] - point2[1])**2)
    
    def _track_persons(self, detections: List[Dict]) -> List[int]:
        """äººå‘˜è·Ÿè¸ªç®—æ³•"""
        current_frame_persons = []
        
        # ä¸ºæ¯ä¸ªæ£€æµ‹åˆ†é…ID
        for detection in detections:
            keypoints = detection['keypoints']
            center = self._get_person_center(keypoints)
            
            if center is None:
                continue
            
            # å¯»æ‰¾æœ€è¿‘çš„å·²è·Ÿè¸ªäººå‘˜
            best_match_id = None
            min_distance = float('inf')
            
            for person_id, person_data in self.tracked_persons.items():
                if person_data.get('missing_frames', 0) < self.max_missing_frames:
                    last_center = person_data.get('last_center')
                    distance = self._calculate_distance(center, last_center)
                    
                    if distance < min_distance and distance < self.tracking_threshold:
                        min_distance = distance
                        best_match_id = person_id
            
            # åˆ†é…ID
            if best_match_id is not None:
                # æ›´æ–°å·²å­˜åœ¨çš„äººå‘˜
                person_id = best_match_id
                self.tracked_persons[person_id]['missing_frames'] = 0
            else:
                # åˆ›å»ºæ–°äººå‘˜
                person_id = self.person_id_counter
                self.person_id_counter += 1
                self.tracked_persons[person_id] = {'missing_frames': 0}
            
            # æ›´æ–°äººå‘˜ä¿¡æ¯
            self.tracked_persons[person_id].update({
                'keypoints': keypoints,
                'last_center': center,
                'bbox': detection.get('bbox'),
                'confidence': detection.get('confidence', 0.0),
                'last_seen_frame': self.frame_count
            })
            
            current_frame_persons.append(person_id)
        
        # å¢åŠ æœªæ£€æµ‹åˆ°çš„äººå‘˜çš„ä¸¢å¤±å¸§æ•°
        for person_id in list(self.tracked_persons.keys()):
            if person_id not in current_frame_persons:
                self.tracked_persons[person_id]['missing_frames'] += 1
                
                # åˆ é™¤ä¸¢å¤±å¤ªä¹…çš„äººå‘˜
                if self.tracked_persons[person_id]['missing_frames'] > self.max_missing_frames:
                    del self.tracked_persons[person_id]
        
        return current_frame_persons
    
    def _draw_skeleton(self, image: np.ndarray, keypoints: np.ndarray, color: Tuple[int, int, int]):
        """ç»˜åˆ¶äººä½“éª¨æ¶"""
        if keypoints is None or len(keypoints) < 51:
            return image
        
        # é‡å¡‘ä¸º (17, 3) å½¢çŠ¶
        kpts = keypoints.reshape(17, 3)
        
        # ç»˜åˆ¶å…³é”®ç‚¹
        for i, (x, y, conf) in enumerate(kpts):
            if conf > 0.3:  # ç½®ä¿¡åº¦é˜ˆå€¼
                cv2.circle(image, (int(x), int(y)), 3, color, -1)
                # å¯é€‰ï¼šæ˜¾ç¤ºå…³é”®ç‚¹ç¼–å·
                # cv2.putText(image, str(i), (int(x), int(y-10)), 
                #            cv2.FONT_HERSHEY_SIMPLEX, 0.3, color, 1)
        
        # ç»˜åˆ¶éª¨æ¶è¿æ¥
        for start_idx, end_idx in self.skeleton_connections:
            if (kpts[start_idx, 2] > 0.3 and kpts[end_idx, 2] > 0.3):
                start_point = (int(kpts[start_idx, 0]), int(kpts[start_idx, 1]))
                end_point = (int(kpts[end_idx, 0]), int(kpts[end_idx, 1]))
                cv2.line(image, start_point, end_point, color, 2)
        
        return image
    
    def _draw_person_info(self, image: np.ndarray, person_id: int, person_data: Dict, 
                         action: str, confidence: float):
        """ç»˜åˆ¶å•ä¸ªäººå‘˜ä¿¡æ¯"""
        keypoints = person_data.get('keypoints')
        center = person_data.get('last_center')
        bbox = person_data.get('bbox')
        detection_conf = person_data.get('confidence', 0.0)
        
        if keypoints is not None and center is not None:
            color = self.colors[person_id % len(self.colors)]
            
            # ç»˜åˆ¶äººä½“éª¨æ¶
            image = self._draw_skeleton(image, keypoints, color)
            
            # ç»˜åˆ¶è¾¹ç•Œæ¡†ï¼ˆå¦‚æœæœ‰ï¼‰
            if bbox is not None:
                x1, y1, x2, y2 = map(int, bbox)
                cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)
            
            # åœ¨äººå‘˜ä¸­å¿ƒç»˜åˆ¶IDå’ŒåŠ¨ä½œä¿¡æ¯
            center_x, center_y = int(center[0]), int(center[1])
            
            # äººå‘˜IDèƒŒæ™¯
            id_text = f"P{person_id+1}"
            text_size = cv2.getTextSize(id_text, cv2.FONT_HERSHEY_SIMPLEX, 0.8, 2)[0]
            cv2.rectangle(image, 
                         (center_x - text_size[0]//2 - 10, center_y - 50),
                         (center_x + text_size[0]//2 + 10, center_y - 20),
                         (0, 0, 0), -1)
            cv2.rectangle(image, 
                         (center_x - text_size[0]//2 - 10, center_y - 50),
                         (center_x + text_size[0]//2 + 10, center_y - 20),
                         color, 2)
            
            # äººå‘˜ID
            cv2.putText(image, id_text, 
                       (center_x - text_size[0]//2, center_y - 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)
            
            # åŠ¨ä½œä¿¡æ¯
            english_action = self.action_mapping.get(action, action)
            action_text = f"{english_action[:8]}"  # é™åˆ¶é•¿åº¦
            
            # åŠ¨ä½œèƒŒæ™¯
            action_size = cv2.getTextSize(action_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]
            cv2.rectangle(image,
                         (center_x - action_size[0]//2 - 5, center_y + 10),
                         (center_x + action_size[0]//2 + 5, center_y + 35),
                         (0, 0, 0), -1)
            cv2.rectangle(image,
                         (center_x - action_size[0]//2 - 5, center_y + 10),
                         (center_x + action_size[0]//2 + 5, center_y + 35),
                         color, 1)
            
            cv2.putText(image, action_text,
                       (center_x - action_size[0]//2, center_y + 28),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
            
            # ç½®ä¿¡åº¦ä¿¡æ¯
            conf_text = f"{confidence:.2f}"
            cv2.putText(image, conf_text,
                       (center_x - 20, center_y + 50),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
    
    def _draw_info_panel(self, image: np.ndarray):
        """ç»˜åˆ¶æ€»ä½“ä¿¡æ¯é¢æ¿"""
        h, w = image.shape[:2]
        
        # ä¸»ä¿¡æ¯é¢æ¿
        panel_height = 140 + len(self.tracked_persons) * 25
        cv2.rectangle(image, (10, 10), (600, panel_height), (0, 0, 0), -1)
        cv2.rectangle(image, (10, 10), (600, panel_height), (255, 255, 255), 2)
        
        # æ ‡é¢˜
        cv2.putText(image, "YOLOv8 Multi-Person Action Recognition", 
                   (20, 35), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)
        
        # FPS
        current_fps = len(self.fps_history) / sum(self.fps_history) if self.fps_history else 0
        cv2.putText(image, f"FPS: {current_fps:.1f}", 
                   (20, 65), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        # è®¾å¤‡ä¿¡æ¯
        cv2.putText(image, f"Device: {self.device.upper()}", 
                   (120, 65), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        # æ´»è·ƒäººæ•°
        active_count = len([p for p in self.tracked_persons.values() if p.get('missing_frames', 0) < 3])
        cv2.putText(image, f"Active: {active_count}", 
                   (220, 65), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        # æ€»è·Ÿè¸ªäººæ•°
        cv2.putText(image, f"Total: {len(self.tracked_persons)}", 
                   (300, 65), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        # æ—¶é—´æˆ³
        current_time = time.strftime("%H:%M:%S", time.localtime())
        cv2.putText(image, current_time, 
                   (400, 65), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        # å¸§è®¡æ•°
        cv2.putText(image, f"Frame: {self.frame_count}", 
                   (500, 65), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        # æ¨¡å‹ä¿¡æ¯
        cv2.putText(image, f"Conf: {self.conf_threshold} | IoU: {self.iou_threshold}", 
                   (20, 95), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 2)
        
        # çŠ¶æ€
        status = f"TRACKING {active_count} PERSONS" if active_count > 0 else "SCANNING..."
        cv2.putText(image, status, 
                   (20, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
        
        return image
    
    def process_frame(self, frame: np.ndarray) -> np.ndarray:
        """å¤„ç†å¸§ï¼ˆYOLOv8å¤šäººæ£€æµ‹ï¼‰"""
        frame_start = time.time()
        
        h, w = frame.shape[:2]
        
        # YOLOv8å§¿æ€æ£€æµ‹
        results = self.yolo_model(frame, conf=self.conf_threshold, iou=self.iou_threshold, verbose=False)
        
        # è§£ææ£€æµ‹ç»“æœ
        detections = []
        
        for result in results:
            if result.keypoints is not None:
                boxes = result.boxes
                keypoints = result.keypoints
                
                for i in range(len(keypoints.data)):
                    # è·å–è¾¹ç•Œæ¡†
                    if boxes is not None and i < len(boxes.data):
                        bbox = boxes.data[i][:4].cpu().numpy()  # x1, y1, x2, y2
                        detection_conf = boxes.data[i][4].cpu().numpy()  # æ£€æµ‹ç½®ä¿¡åº¦
                    else:
                        bbox = None
                        detection_conf = 0.0
                    
                    # è·å–å…³é”®ç‚¹ (17, 3) -> flatten to (51,)
                    kpts = keypoints.data[i].cpu().numpy()  # (17, 3)
                    
                    detections.append({
                        'keypoints': kpts.flatten(),  # è½¬æ¢ä¸ºä¸€ç»´æ•°ç»„
                        'bbox': bbox,
                        'confidence': detection_conf
                    })
        
        # äººå‘˜è·Ÿè¸ª
        active_persons = self._track_persons(detections)
        
        # åŠ¨ä½œè¯†åˆ«å’Œç»˜åˆ¶
        for person_id in active_persons:
            person_data = self.tracked_persons[person_id]
            keypoints = person_data.get('keypoints')
            
            if keypoints is not None:
                # è½¬æ¢ä¸ºCOCOæ ¼å¼ï¼ˆå·²ç»æ˜¯äº†ï¼Œåªéœ€ç¡®ä¿é•¿åº¦ï¼‰
                coco_keypoints = self._yolo_to_coco_keypoints(keypoints)
                
                # åŠ¨ä½œè¯†åˆ«
                # å°†å…³é”®ç‚¹è½¬æ¢ä¸ºnumpyæ•°ç»„æ ¼å¼ (17, 3)
                keypoints_array = np.array(coco_keypoints).reshape(17, 3)
                recognition_results = self.action_recognizer.recognize_actions_single_person(keypoints_array, person_id)
                
                # è·å–æœ€é«˜ç½®ä¿¡åº¦çš„åŠ¨ä½œ
                if recognition_results:
                    action_item = max(recognition_results.items(), key=lambda x: x[1])
                    action_eng = action_item[0]
                    confidence = action_item[1]
                    
                    # è½¬æ¢ä¸ºä¸­æ–‡åŠ¨ä½œå
                    action_cn_map = {'stand': 'ç«™ç«‹', 'sit': 'å', 'lie': 'èººä¸‹', 'drowsy': 'æ‰“çŒç¡'}
                    action = action_cn_map.get(action_eng, 'æœªçŸ¥')
                else:
                    action = 'æœªçŸ¥'
                    confidence = 0.0
                
                # ç»˜åˆ¶äººå‘˜ä¿¡æ¯
                self._draw_person_info(frame, person_id, person_data, action, confidence)
                
                # è°ƒè¯•è¾“å‡º
                english_action = self.action_mapping.get(action, action)
                print(f"Person {person_id+1}: {english_action} (ç½®ä¿¡åº¦: {confidence:.3f}) "
                      f"[æ£€æµ‹ç½®ä¿¡åº¦: {person_data.get('confidence', 0.0):.3f}]")
        
        # ç»˜åˆ¶ä¿¡æ¯é¢æ¿
        frame = self._draw_info_panel(frame)
        
        # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
        frame_time = time.time() - frame_start
        self.fps_history.append(frame_time)
        self.frame_count += 1
        
        return frame
    
    def run_video(self, source):
        """è¿è¡Œè§†é¢‘å¤„ç†ï¼ˆæ”¯æŒæ‘„åƒå¤´IDã€è§†é¢‘æ–‡ä»¶ã€RTSPæµï¼‰"""
        if isinstance(source, int):
            print(f"ğŸ¥ å¯åŠ¨æ‘„åƒå¤´ {source} (YOLOv8å¤šäººæ¨¡å¼)...")
        else:
            print(f"ğŸ¥ å¤„ç†è§†é¢‘: {source} (YOLOv8å¤šäººæ¨¡å¼)...")
        
        cap = cv2.VideoCapture(source)
        if not cap.isOpened():
            raise RuntimeError(f"æ— æ³•æ‰“å¼€è§†é¢‘æº: {source}")
        
        # è·å–è§†é¢‘ä¿¡æ¯
        fps = cap.get(cv2.CAP_PROP_FPS)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        print(f"ğŸ“‹ è§†é¢‘ä¿¡æ¯: {width}x{height}, {fps:.1f}FPS, {total_frames} frames")
        
        self.is_running = True
        print("ğŸ“¸ å¼€å§‹YOLOv8å¤šäººè·Ÿè¸ª...")
        print("ğŸ® æ§åˆ¶: 'q'é€€å‡º, 's'æˆªå›¾, 'r'é‡ç½®è·Ÿè¸ª, 'c'æ¸…é™¤å†å², '='æé«˜ç½®ä¿¡åº¦, '-'é™ä½ç½®ä¿¡åº¦")
        
        try:
            while self.is_running:
                ret, frame = cap.read()
                if not ret:
                    if isinstance(source, str) and source.endswith(('.mp4', '.avi', '.mov', '.mkv')):
                        print("ğŸ“¹ è§†é¢‘æ’­æ”¾å®Œæ¯•")
                        break
                    else:
                        print("âŒ æ— æ³•è¯»å–å¸§")
                        break
                
                # å¤„ç†å¸§
                processed_frame = self.process_frame(frame)
                
                # æ˜¾ç¤º
                cv2.imshow("YOLOv8 Multi-Person Action Recognition", processed_frame)
                
                # é”®ç›˜æ§åˆ¶
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    break
                elif key == ord('s'):
                    # æˆªå›¾
                    timestamp = int(time.time())
                    filename = f"yolo_multiperson_{timestamp}.jpg"
                    cv2.imwrite(filename, processed_frame)
                    print(f"ğŸ“· æˆªå›¾ä¿å­˜: {filename}")
                elif key == ord('r'):
                    # é‡ç½®è·Ÿè¸ª
                    self.tracked_persons.clear()
                    self.person_id_counter = 0
                    print("ğŸ”„ äººå‘˜è·Ÿè¸ªå·²é‡ç½®")
                elif key == ord('c'):
                    # æ¸…é™¤å†å²
                    self.fps_history.clear()
                    self.frame_count = 0
                    self.start_time = time.time()
                    print("ğŸ—‘ï¸ å†å²æ•°æ®å·²æ¸…é™¤")
                elif key == ord('=') or key == ord('+'):
                    # æé«˜ç½®ä¿¡åº¦
                    self.conf_threshold = min(0.9, self.conf_threshold + 0.05)
                    print(f"ğŸ“ˆ ç½®ä¿¡åº¦é˜ˆå€¼: {self.conf_threshold:.2f}")
                elif key == ord('-'):
                    # é™ä½ç½®ä¿¡åº¦
                    self.conf_threshold = max(0.1, self.conf_threshold - 0.05)
                    print(f"ğŸ“‰ ç½®ä¿¡åº¦é˜ˆå€¼: {self.conf_threshold:.2f}")
        
        finally:
            self.is_running = False
            cap.release()
            cv2.destroyAllWindows()
            print("ğŸ“Š YOLOv8è¯†åˆ«ç»Ÿè®¡:")
            print(f"   æ€»å¸§æ•°: {self.frame_count}")
            print(f"   è¿è¡Œæ—¶é—´: {time.time() - self.start_time:.2f}s")
            print(f"   è·Ÿè¸ªè¿‡çš„äººæ•°: {self.person_id_counter}")
            if self.frame_count > 0:
                avg_fps = self.frame_count / (time.time() - self.start_time)
                print(f"   å¹³å‡FPS: {avg_fps:.2f}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¤– YOLOv8å¤šäººå§¿æ€åŠ¨ä½œè¯†åˆ«ç³»ç»Ÿ")
    print("âœ… æ›´å‡†ç¡®çš„å¤šäººæ£€æµ‹") 
    print("âœ… åŸç”ŸCOCOå…³é”®ç‚¹æ ¼å¼")
    print("âœ… GPUåŠ é€Ÿæ”¯æŒ")
    print("=" * 50)
    
    # å¯é€‰æ‹©ä¸åŒçš„YOLOv8æ¨¡å‹
    # yolov8n-pose.pt (æœ€å¿«)
    # yolov8s-pose.pt (å¹³è¡¡)  
    # yolov8m-pose.pt (æ›´å‡†ç¡®)
    # yolov8l-pose.pt (æœ€å‡†ç¡®)
    
    recognizer = YOLOMultiPersonRecognizer(model_path='yolo11l-pose.pt')
    
    try:
        # å¯ä»¥ä½¿ç”¨æ‘„åƒå¤´ã€è§†é¢‘æ–‡ä»¶æˆ–RTSPæµ
        # recognizer.run_video(0)  # æ‘„åƒå¤´
        recognizer.run_video("11.mp4")  # è§†é¢‘æ–‡ä»¶
        # recognizer.run_video("rtsp://...")  # RTSPæµ
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    finally:
        print("ğŸ YOLOv8è¯†åˆ«ç¨‹åºç»“æŸ")


if __name__ == '__main__':
    main() 